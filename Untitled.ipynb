{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92ba7b85-d629-41b3-8db6-caddd9e95d6e",
   "metadata": {},
   "source": [
    "# Coding the Real Transformer from Scratch \n",
    "*(Understand the basic 1 encoder and 1 decoder without masking transformer FIRST.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7c2b9c-129b-47a8-b8da-543a2af22132",
   "metadata": {},
   "source": [
    "<img src=\"transformer images/transformer_model.PNG\" alt=\"Sample Image\" style=\"width:1000px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925ec349-1195-4486-88ad-10bd286ee3a3",
   "metadata": {},
   "source": [
    "# Input Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788e0f6e-d267-4fa2-a5e6-15c1ba9d2fa0",
   "metadata": {},
   "source": [
    "<img src=\"transformer images/embedding.PNG\" alt=\"Sample Image\" style=\"width:1000px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dc5c477-62fe-45f4-af31-2fdefd1a923f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "598ef3cb-56b3-423a-a6ea-1d9bd7e29724",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self,d_model,vocab_size): # d_model : dimensions , vocab_size : How many words are there in the vocabulary.\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size,d_model)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model) # This is in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b31a653-3cf2-4126-bbe3-1dd2dbc6ead5",
   "metadata": {},
   "source": [
    "```python\n",
    "vocab = {\"hello\": 0, \",\": 1, \"there\": 2}\n",
    "sent = \"hello , there\"  # Input sentence\n",
    "tokenized = [vocab[word] for word in sent.split()]  # Convert words to indices\n",
    "input_tensor = torch.tensor(tokenized)\n",
    "ie = InputEmbeddings(d_model=512, vocab_size=len(vocab))\n",
    "output = ie.forward(input_tensor)\n",
    "print(output.shape)\n",
    "```\n",
    "You can run this to see for yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780d41ac-e581-48ba-a48e-c7a6d4f59260",
   "metadata": {},
   "source": [
    "# Positional Encoding\n",
    "<img src=\"transformer images/positional_encoding.PNG\" alt=\"Sample Image\" style=\"width:1000px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2134d75-9a95-4d25-a04f-8b251bfc7335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_model : We are creating another similar vector of size same as before.\n",
    "# seq_len : Maximum length for the SENTENC, BECAUSE WE NEED TO CREATE 1 VECTOR FOR EACH POSITION AS IN PIC ABOVE.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,d_model,seq_len,dropout):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # First we will build a matrix of seq_len,d_model. Because we need 512 features for each token. and total token is seq_len.\n",
    "        pe = torch.zeros(seq_len,d_model)\n",
    "        # create a position matrix that will represent the position of the token inside the sentence --> seq_len , 1\n",
    "        position = torch.arange(0,seq_len,dtype=torch.float).unsqueeze(1) # --> (seq_len,1)\n",
    "        div_term = torch.exp(torch.arange(0,d_model,2).float() * (-math.log(10000.0) / d_model))\n",
    "        # apply the sin to even and cos to odd position\n",
    "        pe[:,0::2] = torch.sin(position * div_term)\n",
    "        pe[:,1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # If you the pe to be saved when you save the model, we use register_buffer.\n",
    "        self.register_buffer(\"pe\",pe)\n",
    "        \n",
    "        pe = pe.unsqueeze(0) # (1,seq_len,d_model)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x + (self.pe[:,:x.shape[1],:]).requires_grad(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dbac39-f48b-4e36-9613-b72ddb56ebd1",
   "metadata": {},
   "source": [
    "# Layer Normalization\n",
    "<img src=\"transformer images/ln.PNG\" alt=\"Sample Image\" style=\"width:1000px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40a5b872-6b34-4c3b-8caf-6791af2a527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self,eps=10**-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps ## Here eps is important because, if the denomitor is 0 , the the x will be undefined or very huge.\n",
    "        self.alpha = nn.Parameter(torch.ones(1)) # Multiplied\n",
    "        self.bias = nn.Parameter(torch.zeros(0)) # Added\n",
    "\n",
    "    def forward(self,x):\n",
    "        mean = x.mean(dim=-1,keepdim= True)\n",
    "        std = x.std(dim=-1,keepdim=True)\n",
    "        return self.alpha * (x-mean) /(std+self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e1c30d-b3a9-4c65-a440-e9dd1b0d3a6b",
   "metadata": {},
   "source": [
    "# Feed Forward Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e3058f8-d744-4e2f-9a70-240df262012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self,d_model,ff_dim,dropout):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model,ff_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(ff_dim,d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.linear_2(self.dropout(self.relu(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f26e844-765f-43f6-8aa6-6ec6f21d19af",
   "metadata": {},
   "source": [
    "# Multihead Attention\n",
    "<img src=\"transformer images/Multiatt.PNG\" alt=\"Sample Image\" style=\"width:1000px;\">\n",
    "\n",
    "## Points to be noted. \n",
    "- We need to divide the d_model with number of head.\n",
    "- self.d_k : See the picture above , the small matrices are dk , this means , if we are using multi head attention then each head will see the whole sentence but different part of the dimension from 512. Basically it is the size of that matrix.\n",
    "- If we want some words to not to interact with other words , we mask them.\n",
    "- As you can see , the formula for attention , we sqrt(dk) there , that means we are calculating it for head.\n",
    "- So if we want some words to not to interact with others , we set them a very small value , so that after softmax , it will be nearly zero , and the model will not focus on those much.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c927f9-b34a-430c-a318-13231e0d375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self,d_model,h,dropout): # h : It is the number of heads we want.\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        assert d_model % h ==0 , \"d_model is not divisible by h\"\n",
    "    \n",
    "        self.d_k = d_model // h\n",
    "        self.w_q = nn.Linear(d_model,d_model)\n",
    "        self.w_k = nn.Linear(d_model,d_model)\n",
    "        self.w_v = nn.Linear(d_model,d_model)\n",
    "    \n",
    "        self.w_o = nn.Linear(d_model,d_model)\n",
    "        self.dropuout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,q,k,v,mask):\n",
    "        query = self.w_q(q) # (Batch , seq_len , d_model) --> (Batch , seq_len , d_model)\n",
    "        key = self.w_k(k)\n",
    "        value = self.w_v(v)\n",
    "\n",
    "        # batch,seq_len,d_model --> batch,seq_len,h,d_k --> batch , h , seq_len, d_k , This means each head will see the whole sentence but a smaller part of the embedding.\n",
    "        query = query.view(query.shape[0],query.shape[1],self.h,self.d_k).transpose(1,2)\n",
    "        key = key.view(key.shape[0],key.shape[1],self.h,self.d_k).transpose(1,2)\n",
    "        value = value.view(value.shape[0],value.shape[1],self.h,self.d_k).transpose(1,2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_39_env",
   "language": "python",
   "name": "pytorch_39_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
