{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05f703b0-152e-43cc-942f-6ebc55492a72",
   "metadata": {},
   "source": [
    "# Transformer Architecture Flow Documentation\n",
    "\n",
    "## Building the Transformer (build_transformer function)\n",
    "\n",
    "### Input Parameters:\n",
    "- src_vocab_size: Size of source vocabulary\n",
    "- tgt_vocab_size: Size of target vocabulary\n",
    "- src_seq_len: Maximum length of source sequence\n",
    "- tgt_seq_len: Maximum length of target sequence\n",
    "- d_model: Dimension of the model (default: 512)\n",
    "- N: Number of encoder/decoder blocks (default: 6)\n",
    "- h: Number of attention heads (default: 8)\n",
    "- dropout: Dropout rate (default: 0.1)\n",
    "- d_ff: Dimension of feed-forward network (default: 2048)\n",
    "\n",
    "### Construction Flow:\n",
    "1. Create embedding layers:\n",
    "   - Source embedding: (batch_size, src_seq_len) → (batch_size, src_seq_len, d_model)\n",
    "   - Target embedding: (batch_size, tgt_seq_len) → (batch_size, tgt_seq_len, d_model)\n",
    "\n",
    "2. Create positional encoding layers:\n",
    "   - Adds positional information to embeddings\n",
    "   - Output shape remains same as input: (batch_size, seq_len, d_model)\n",
    "   - Uses sine and cosine functions for different frequencies\n",
    "\n",
    "3. Build Encoder:\n",
    "   - Creates N identical encoder blocks\n",
    "   - Each encoder block contains:\n",
    "     a. Self-attention block (MultiHeadAttentionBlock)\n",
    "     b. Feed-forward block\n",
    "     c. Two residual connections with layer normalization\n",
    "\n",
    "4. Build Decoder:\n",
    "   - Creates N identical decoder blocks\n",
    "   - Each decoder block contains:\n",
    "     a. Self-attention block\n",
    "     b. Cross-attention block\n",
    "     c. Feed-forward block\n",
    "     d. Three residual connections with layer normalization\n",
    "\n",
    "5. Create projection layer:\n",
    "   - Converts decoder output to vocabulary probabilities\n",
    "   - Input: (batch_size, seq_len, d_model)\n",
    "   - Output: (batch_size, seq_len, tgt_vocab_size)\n",
    "\n",
    "## Forward Pass Flow\n",
    "\n",
    "### 1. Encoding Process:\n",
    "```\n",
    "Input → Embedding → Positional Encoding → Encoder Blocks → Encoder Output\n",
    "(batch, seq_len) → (batch, seq_len, d_model) → (batch, seq_len, d_model) → (batch, seq_len, d_model)\n",
    "```\n",
    "\n",
    "#### Encoder Block Processing:\n",
    "1. Self-Attention:\n",
    "   - Input splits into Q, K, V: (batch, seq_len, d_model) → (batch, h, seq_len, d_k)\n",
    "   - Attention computation: (batch, h, seq_len, seq_len)\n",
    "   - Output: (batch, seq_len, d_model)\n",
    "2. Feed-Forward:\n",
    "   - First linear: (batch, seq_len, d_model) → (batch, seq_len, d_ff)\n",
    "   - ReLU activation\n",
    "   - Second linear: (batch, seq_len, d_ff) → (batch, seq_len, d_model)\n",
    "\n",
    "### 2. Decoding Process:\n",
    "```\n",
    "Target → Embedding → Positional Encoding → Decoder Blocks → Projection → Output Probabilities\n",
    "(batch, seq_len) → (batch, seq_len, d_model) → (batch, seq_len, d_model) → (batch, seq_len, tgt_vocab_size)\n",
    "```\n",
    "\n",
    "#### Decoder Block Processing:\n",
    "1. Self-Attention (masked):\n",
    "   - Prevents attending to future tokens\n",
    "   - Same shape transformations as encoder self-attention\n",
    "2. Cross-Attention:\n",
    "   - Q: from decoder, K,V: from encoder output\n",
    "   - Allows decoder to focus on relevant parts of input sequence\n",
    "3. Feed-Forward:\n",
    "   - Identical to encoder feed-forward network\n",
    "\n",
    "### 3. Multi-Head Attention Details:\n",
    "1. Linear projections:\n",
    "   ```\n",
    "   Input(batch, seq_len, d_model) → \n",
    "   Split into h heads(batch, h, seq_len, d_k) →\n",
    "   Attention(batch, h, seq_len, seq_len) →\n",
    "   Values(batch, h, seq_len, d_k) →\n",
    "   Concat(batch, seq_len, d_model)\n",
    "   ```\n",
    "\n",
    "2. Attention formula:\n",
    "   ```\n",
    "   Attention(Q,K,V) = softmax(QK^T/√d_k)V\n",
    "   ```\n",
    "\n",
    "### 4. Important Shape Transformations:\n",
    "- Source input: (batch_size, src_seq_len)\n",
    "- Target input: (batch_size, tgt_seq_len)\n",
    "- Embeddings: (..., d_model)\n",
    "- Attention heads: (batch_size, num_heads, seq_len, d_k)\n",
    "- Feed-forward: (batch_size, seq_len, d_ff)\n",
    "- Final output: (batch_size, tgt_seq_len, tgt_vocab_size)\n",
    "\n",
    "## Training Flow:\n",
    "1. Forward pass:\n",
    "   - Source sequence → Encoder → Encoder Output\n",
    "   - Target sequence → Decoder (using encoder output) → Predictions\n",
    "2. Loss calculation:\n",
    "   - Compare predictions with shifted target sequence\n",
    "3. Backward pass:\n",
    "   - Gradient calculation and parameter updates\n",
    "4. Key masks:\n",
    "   - Source mask: Handles padding in input sequence\n",
    "   - Target mask: Prevents attending to future tokens (causal mask)\n",
    "\n",
    "## Implementation Notes:\n",
    "- Layer normalization is applied before attention and feed-forward (pre-norm)\n",
    "- Residual connections help with gradient flow\n",
    "- Xavier uniform initialization is used for all parameters\n",
    "- Dropout is applied:\n",
    "  1. After positional encoding\n",
    "  2. After attention softmax\n",
    "  3. After each sublayer before residual connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2309bdf-849f-4c91-89fd-d29ff4fee7f9",
   "metadata": {},
   "source": [
    "# Detailed Transformer Execution Flow - Step by Step\r\n",
    "\r\n",
    "## 1. Initial Construction (build_transformer function call)\r\n",
    "\r\n",
    "### 1.1 Component Creation\r\n",
    "```python\r\n",
    "transformer = build_transformer(\r\n",
    "    src_vocab_size=32000,  # example values\r\n",
    "    tgt_vocab_size=32000,\r\n",
    "    src_seq_len=512,\r\n",
    "    tgt_seq_len=512\r\n",
    ")\r\n",
    "```\r\n",
    "\r\n",
    "1. Creates embedding layers:\r\n",
    "   ```python\r\n",
    "   src_embed = InputEmbeddings(d_model, src_vocab_size)\r\n",
    "   tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\r\n",
    "   ```\r\n",
    "\r\n",
    "2. Creates positional encoding layers:\r\n",
    "   ```python\r\n",
    "   src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\r\n",
    "   tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\r\n",
    "   ```\r\n",
    "   - During initialization, creates position encodings matrix (pe):\r\n",
    "     - Shape: (1, seq_len, d_model)\r\n",
    "     - Contains sine/cosine patterns for each position\r\n",
    "\r\n",
    "3. Creates N encoder blocks and N decoder blocks\r\n",
    "4. Initializes projection layer\r\n",
    "\r\n",
    "## 2. Forward Pass Execution Flow\r\n",
    "\r\n",
    "When you input source and target sequences:\r\n",
    "```python\r\n",
    "output = transformer(src_sequence, tgt_sequence, src_mask, tgt_mask)\r\n",
    "```\r\n",
    "\r\n",
    "### 2.1 Source Input Processing (Encoding)\r\n",
    "\r\n",
    "1. **Embedding Layer** (`src_embed`):\r\n",
    "   ```python\r\n",
    "   # Input: (batch_size, src_seq_len)\r\n",
    "   embedded = self.src_embed(src)\r\n",
    "   # Output: (batch_size, src_seq_len, d_model)\r\n",
    "   ```\r\n",
    "\r\n",
    "2. **Positional Encoding** (`src_pos`):\r\n",
    "   ```python\r\n",
    "   # Forward method of PositionalEncoding\r\n",
    "   position_encoded = self.src_pos(embedded)\r\n",
    "   # Adds pre-computed positional encodings to embeddings\r\n",
    "   # Output: (batch_size, src_seq_len, d_model)\r\n",
    "   ```\r\n",
    "\r\n",
    "3. **Encoder Processing**:\r\n",
    "   ```python\r\n",
    "   encoder_output = self.encoder(position_encoded, src_mask)\r\n",
    "   ```\r\n",
    "   \r\n",
    "   For each encoder block:\r\n",
    "   \r\n",
    "   a. **Self-Attention** (`MultiHeadAttentionBlock`):\r\n",
    "      ```python\r\n",
    "      # 1. Linear projections\r\n",
    "      Q = self.w_q(x)  # (batch, seq_len, d_model)\r\n",
    "      K = self.w_k(x)  # (batch, seq_len, d_model)\r\n",
    "      V = self.w_v(x)  # (batch, seq_len, d_model)\r\n",
    "      \r\n",
    "      # 2. Reshape for multiple heads\r\n",
    "      Q = Q.view(batch, seq_len, h, d_k).transpose(1, 2)\r\n",
    "      # Now: (batch, h, seq_len, d_k)\r\n",
    "      \r\n",
    "      # 3. Attention computation\r\n",
    "      attention_scores = (Q @ K.transpose(-2, -1)) / math.sqrt(d_k)\r\n",
    "      # Apply mask and softmax\r\n",
    "      attention_output = attention_scores @ V\r\n",
    "      \r\n",
    "      # 4. Concatenate heads and project\r\n",
    "      attention_output = self.w_o(attention_output)\r\n",
    "      ```\r\n",
    "\r\n",
    "   b. **Feed-Forward Network**:\r\n",
    "      ```python\r\n",
    "      # Two linear transformations with ReLU\r\n",
    "      ff_output = self.linear_2(self.dropout(\r\n",
    "          torch.relu(self.linear_1(attention_output))\r\n",
    "      ))\r\n",
    "      ```\r\n",
    "\r\n",
    "### 2.2 Target Input Processing (Decoding)\r\n",
    "\r\n",
    "1. **Embedding Layer** (`tgt_embed`):\r\n",
    "   ```python\r\n",
    "   # Input: (batch_size, tgt_seq_len)\r\n",
    "   embedded = self.tgt_embed(tgt)\r\n",
    "   # Output: (batch_size, tgt_seq_len, d_model)\r\n",
    "   ```\r\n",
    "\r\n",
    "2. **Positional Encoding** (`tgt_pos`):\r\n",
    "   ```python\r\n",
    "   position_encoded = self.tgt_pos(embedded)\r\n",
    "   ```\r\n",
    "\r\n",
    "3. **Decoder Processing**:\r\n",
    "   For each decoder block:\r\n",
    "   \r\n",
    "   a. **Masked Self-Attention**:\r\n",
    "      - Same as encoder self-attention but with causal mask\r\n",
    "      - Prevents attending to future positions\r\n",
    "   \r\n",
    "   b. **Cross-Attention**:\r\n",
    "      ```python\r\n",
    "      # Q from decoder, K,V from encoder\r\n",
    "      cross_attention = self.cross_attention_block(\r\n",
    "          decoder_output,  # Q\r\n",
    "          encoder_output,  # K\r\n",
    "          encoder_output   # V\r\n",
    "      )\r\n",
    "      ```\r\n",
    "   \r\n",
    "   c. **Feed-Forward Network**:\r\n",
    "      - Same structure as encoder\r\n",
    "\r\n",
    "### 2.3 Final Projection\r\n",
    "\r\n",
    "```python\r\n",
    "# Input: (batch_size, tgt_seq_len, d_model)\r\n",
    "output = self.projection_layer(decoder_output)\r\n",
    "# Output: (batch_size, tgt_seq_len, tgt_vocab_size)\r\n",
    "```\r\n",
    "\r\n",
    "## 3. Residual Connections and Layer Normalization\r\n",
    "\r\n",
    "Throughout the network:\r\n",
    "\r\n",
    "```python\r\n",
    "class ResidualConnection(nn.Module):\r\n",
    "    def forward(self, x, sublayer):\r\n",
    "        # 1. Layer normalization\r\n",
    "        normalized = self.norm(x)\r\n",
    "        # 2. Apply sublayer (attention or feed-forward)\r\n",
    "        sublayer_output = sublayer(normalized)\r\n",
    "        # 3. Apply dropout\r\n",
    "        dropout_output = self.dropout(sublayer_output)\r\n",
    "        # 4. Add residual connection\r\n",
    "        return x + dropout_output\r\n",
    "```\r\n",
    "\r\n",
    "## 4. Training Process\r\n",
    "\r\n",
    "1. Forward pass triggers when loss is calculated:\r\n",
    "   ```python\r\n",
    "   # Example training step\r\n",
    "   optimizer.zero_grad()\r\n",
    "   output = transformer(src, tgt_input, src_mask, tgt_mask)\r\n",
    "   loss = criterion(output, tgt_output)\r\n",
    "   ```\r\n",
    "\r\n",
    "2. Backward pass:\r\n",
    "   ```python\r\n",
    "   loss.backward()\r\n",
    "   optimizer.step()\r\n",
    "   ```\r\n",
    "\r\n",
    "## 5. Mask Types and Creation\r\n",
    "\r\n",
    "1. **Source Padding Mask**:\r\n",
    "   - Created for source sequences\r\n",
    "   - Masks padding tokens (usually 0s)\r\n",
    "   ```python\r\n",
    "   # Shape: (batch_size, 1, 1, src_seq_len)\r\n",
    "   src_mask = (src != pad_token).unsqueeze(1).unsqueeze(2)\r\n",
    "   ```\r\n",
    "\r\n",
    "2. **Target Causal Mask**:\r\n",
    "   - Combines padding mask and causal mask\r\n",
    "   - Prevents attending to future tokens\r\n",
    "   ```python\r\n",
    "   # Shape: (batch_size, 1, tgt_seq_len, tgt_seq_len)\r\n",
    "   tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\r\n",
    "   ```\r\n",
    "\r\n",
    "## 6. Memory Flow and Attention Patterns\r\n",
    "\r\n",
    "1. **Encoder Self-Attention**:\r\n",
    "   - Each position can attend to all positions in the source sequence\r\n",
    "   - Memory complexity: O(n²) where n is sequence length\r\n",
    "\r\n",
    "2. **Decoder Self-Attention**:\r\n",
    "   - Each position can only attend to previous positions\r\n",
    "   - Memory complexity: O(n²)\r\n",
    "\r\n",
    "3. **Cross-Attention**:\r\n",
    "   - Each decoder position can attend to all encoder positions\r\n",
    "   - Memory complexity: O(n²)\r\n",
    "\r\n",
    "## 7. Dimension Transitions Throughout Network\r\n",
    "\r\n",
    "```\r\n",
    "Input → Embedding → Positional → Attention → FF → Output\r\n",
    "(L) → (L,D) → (L,D) → (L,D) → (L,D) → (L,V)\r\n",
    "\r\n",
    "Where:\r\n",
    "L = sequence length\r\n",
    "D = d_model (512 default)\r\n",
    "V = vocabulary size\r\n",
    "```\r\n",
    "\r\n",
    "Each attention head: D/h dimensions (64 if d_model=512 and h=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e44d27e-3f40-4c78-8834-a4dd20916ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "class InputEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None: # d_model : dimensions , vocab_size : How many words are there in the vocabulary.\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len) --> (batch, seq_len, d_model)\n",
    "        # Multiply by sqrt(d_model) to scale the embeddings according to the paper\n",
    "        return self.embedding(x) * math.sqrt(self.d_model) # This is in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81d903b9-6928-4c19-871e-6a44cd38a02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [3, 4, 5, 6]\n",
      "Token IDs with <sos> and <eos>: [1, 3, 4, 5, 6, 2]\n"
     ]
    }
   ],
   "source": [
    "vocab = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"I\": 3, \"am\": 4, \"learning\": 5, \"transformers\": 6}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Reverse vocabulary for token-to-text mapping\n",
    "reverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "# Example tokenizer\n",
    "def tokenize(text):\n",
    "    return [vocab[token] for token in text.split() if token in vocab]\n",
    "import torch\n",
    "\n",
    "# Input sentence\n",
    "text = \"I am learning transformers\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "token_ids = tokenize(text)\n",
    "print(f\"Token IDs: {token_ids}\")  # Output: [3, 4, 5, 6]\n",
    "\n",
    "# Add <sos> and <eos> tokens\n",
    "token_ids = [vocab[\"<sos>\"]] + token_ids + [vocab[\"<eos>\"]]\n",
    "print(f\"Token IDs with <sos> and <eos>: {token_ids}\")  # Output: [1, 3, 4, 5, 6, 2]\n",
    "\n",
    "# Convert to tensor\n",
    "input_ids = torch.tensor([token_ids])  # Shape: (batch_size=1, seq_len=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a73e5392-14cd-4380-b83c-419abcab54d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "\n",
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)\n",
    "d_model = 8  # Example embedding dimension\n",
    "embedding_layer = InputEmbeddings(d_model, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acd6605b-9e6f-4456-b05e-186a591f6b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.3973, -0.3792,  2.3362,  1.0594,  5.3727,  0.4745, -1.6741,\n",
       "           0.0453],\n",
       "         [ 3.4330, -0.0212,  3.6404, -0.4299, -3.4045,  1.3409, -1.3734,\n",
       "           2.6941],\n",
       "         [ 0.1255,  1.5592, -2.0894,  2.4839, -0.4168,  2.5544, -3.7248,\n",
       "           0.7863],\n",
       "         [ 0.0383, -3.0224, -1.6236,  5.3485,  3.2719, -2.2278,  1.6502,\n",
       "           2.0719],\n",
       "         [-0.7482,  0.5400, -1.1554, -3.9622,  4.8737, -5.5352,  0.0768,\n",
       "           0.8087],\n",
       "         [-0.2199, -0.3985, -3.0491,  4.4211,  5.0618,  0.3720, -3.8904,\n",
       "           0.9432]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7909533d-3072-4394-b9f9-5e59c1922cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a43d255-c244-4b57-9f79-5935b33c2c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 1.0000e-01, 1.0000e-02, 1.0000e-03])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5c5d7df-35ee-4f01-bb7a-945a204b9ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = torch.zeros(3, 512)\n",
    "# Create a vector of shape (seq_len)\n",
    "position = torch.arange(0, 3, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
    "# Create a vector of shape (d_model)\n",
    "div_term = torch.exp(torch.arange(0, 512, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
    "# Apply sine to even indices\n",
    "pe[:, 0::2] = torch.sin(position * div_term)\n",
    "pe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3527a602-de68-4bac-8915-840b86cb7270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div_term.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3ba97b0-e87c-47d3-a6c8-061999bf46fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ba1f267-2c6d-4722-9621-95c5489425bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.sqrt(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b4c2f63-5fed-4678-8b35-e14500a0feb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.125, 0.0625, 0.0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2/16 , 1/16 , 0/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fcb5d86e-62a3-481a-a495-0f0c67c75aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(1,2,3,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92044c42-40b9-4d4b-a339-5c8b14d7c704",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def causal_mask(size):\n",
    "    mask = torch.triu(torch.ones(1,size,size),diagonal=1).type(torch.int)\n",
    "    return mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45a5d8c3-4a1f-44ea-bc78-12aa34ddc3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1, 15,  6,  2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example token values\n",
    "seq_len = 4\n",
    "self_sos_token = torch.tensor([1], dtype=torch.int64)  # start of sequence token\n",
    "dec_input_tokens = [15,6]  # the input tokens\n",
    "self_pad_token = 0  # padding token\n",
    "dec_num_padding_tokens = seq_len - len(dec_input_tokens) - 2  # number of padding tokens to add\n",
    "self_eos_token = torch.tensor([2], dtype=torch.int64)\n",
    "\n",
    "# Concatenate the tokens\n",
    "decoder_input = torch.cat(\n",
    "    [\n",
    "        self_sos_token,\n",
    "        torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "        torch.tensor([self_pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "        self_eos_token\n",
    "    ],\n",
    "    dim=0,\n",
    ")\n",
    "\n",
    "print(decoder_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f62fa25a-f759-4f0f-83e9-1132aadae075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(decoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31418158-82c7-442a-b2cf-3922b1e26f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6c2bf13-58a4-441c-93bf-9fc7b7b8f8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_pre_mask = (decoder_input != self_pad_token).unsqueeze(0).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cfbb1f81-d733-4376-b4c5-4a35a9f11e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_pre_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f99ded79-1bb0-4c38-acf6-57d100ac9593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee5dbaeb-5d20-4171-ad2b-543f7e8d792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = causal_mask(decoder_input.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26baf0c9-40c8-40c4-a077-5ec75571a4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 0]], dtype=torch.int32)\n",
      "tensor([[[ True, False, False, False],\n",
      "         [ True,  True, False, False],\n",
      "         [ True,  True,  True, False],\n",
      "         [ True,  True,  True,  True]]])\n",
      "tensor([[[1, 0, 0, 0],\n",
      "         [1, 1, 0, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 1, 0]]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# returns = {\"decoder_mask\": (decoder_input != self_pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0))} # (1, seq_len) & (1, seq_len, seq_len),\n",
    "print(decoder_pre_mask)\n",
    "print(cm)\n",
    "\n",
    "print(decoder_pre_mask & cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50148994-fa97-42c8-bfca-9cecdea824a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconoda\\envs\\pytorch_39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader,random_split\n",
    "def get_or_build_tokenizer(config,ds,lang):\n",
    "    # config['tokenizer_file'] = '..tokenizers/tokenizer_{0}.json'\n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token='[UNK]'))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\",\"[PAD]\",\"[SOS]\",\"[EOS]\"], min_frequency=2) # min_frequency A word to have it in our vocab , the frequency of that word must be 2.\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds,lang),trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer\n",
    "class BilingualDataset(Dataset):\n",
    "\n",
    "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "\n",
    "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_target_pair = self.ds[idx]\n",
    "        src_text = src_target_pair['translation'][self.src_lang]\n",
    "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
    "\n",
    "        # Transform the text into tokens\n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "        # Add sos, eos and padding to each sentence\n",
    "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2  # We will add <s> and </s>\n",
    "        # We will only add <s>, and </s> only on the label\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1 # we will add only <s> at the start. So to skip that we did -1\n",
    "\n",
    "        # Make sure the number of padding tokens is not negative. If it is, the sentence is too long\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError(\"Sentence is too long\")\n",
    "\n",
    "        # Add <s> and </s> token\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Add only <s> token\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Add only </s> token\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Double check the size of the tensors to make sure they are all seq_len long\n",
    "        assert encoder_input.size(0) == self.seq_len\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert label.size(0) == self.seq_len\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input,  # (seq_len)\n",
    "            \"decoder_input\": decoder_input,  # (seq_len)\n",
    "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)\n",
    "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # (1, seq_len) & (1, seq_len, seq_len),\n",
    "            \"label\": label,  # (seq_len)\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text,\n",
    "        }\n",
    "\n",
    "def causal_mask(size):\n",
    "    mask = torch.triu(torch.ones(1,size,size),diagonal=1).type(torch.int)\n",
    "    return mask == 0\n",
    "\n",
    "\n",
    "\n",
    "def get_ds(config):\n",
    "    ds_raw = load_dataset('opus_books',f\"{config['lang_src']}-{config['lang_tgt']}\",split='train')\n",
    "\n",
    "    # build the tokenizer\n",
    "    tokenizer_src = get_or_build_tokenizer(config,ds_raw,config['lang_src'])\n",
    "    tokenizer_tgt = get_or_build_tokenizer(config,ds_raw,config['lang_tgt'])\n",
    "\n",
    "    # keep 90% for training and 10% for validation\n",
    "    train_ds_size = int(0.9 * len(ds_raw))\n",
    "    val_ds_size = len(ds_raw) - train_ds_size\n",
    "    train_ds_raw , val_ds_raw = random_split(ds_raw,[train_ds_size,val_ds_size])\n",
    "\n",
    "    print(\"train_ds_raw\",next(iter(train_ds_raw)))\n",
    "    # train_ds and val_ds\n",
    "    train_ds = BilingualDataset(train_ds_raw,tokenizer_src,tokenizer_tgt,config['lang_src'],config['lang_tgt'],config['seq_len'])\n",
    "    val_ds  = BilingualDataset(val_ds_raw,tokenizer_src,tokenizer_tgt,config['lang_src'],config['lang_tgt'],config['seq_len'])\n",
    "    max_len_src = 0\n",
    "    max_len_tgt = 0\n",
    "    for item in ds_raw:\n",
    "        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
    "        tgt_ids = tokenizer_src.encode(item['translation'][config['lang_tgt']]).ids\n",
    "        max_len_src = max(max_len_src , len(src_ids))\n",
    "        max_len_tgt = max(max_len_tgt,len(tgt_ids))\n",
    "    print(f\"Max length of Soure sentence : {max_len_src}\")\n",
    "    print(f\"Max length of target sentence : {max_len_tgt}\")\n",
    "\n",
    "    train_dataloader = DataLoader(train_ds,batch_size=config['batch_size'],shuffle= True)\n",
    "    val_dataloader = DataLoader(val_ds, batch_size = 1 , shuffle = True)\n",
    "    print(train_dataloader)\n",
    "    return train_dataloader , val_dataloader, tokenizer_src , tokenizer_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92a10e30-4ab7-4745-805a-7352487a084d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile config2.py\n",
    "from pathlib import Path\n",
    "def get_config():\n",
    "    return {\n",
    "        \"batch_size\" : 8,\n",
    "        \"num_epochs\" : 20,\n",
    "        \"lr\" : 10**-4,\n",
    "        \"seq_len\" : 350,\n",
    "        \"d_model\" : 512,\n",
    "        \"lang_src\" : \"en\",\n",
    "        \"lang_tgt\" : \"it\",\n",
    "        \"model_folder\" : \"weights\",\n",
    "        \"model_basename\" : \"tmodel_\",\n",
    "        \"preload\" : None,\n",
    "        \"tokenizer_file\" : \"tokenizer_{0}.json\",\n",
    "        \"experiment_name\" : \"runs/tmodel\"\n",
    "    }\n",
    "def get_weights_file_path(config,epoch:str):\n",
    "    model_folder = config['model_folder']\n",
    "    model_basename = config['model_basename']\n",
    "    model_filename = f\"{model_basename}{epoch}.pt\"\n",
    "    return str(Path('.') / model_folder / model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f9b152b-140a-49e1-9a8f-028e36f5ae6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ds_raw {'id': '14822', 'translation': {'en': 'And every gentle air that dallied,', 'it': 'E, ad ogni dolce venticello'}}\n",
      "Max length of Soure sentence : 309\n",
      "Max length of target sentence : 274\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x0000027977B2B430>\n"
     ]
    }
   ],
   "source": [
    "from config2 import *\n",
    "config = get_config()\n",
    "train_dataloader , val_dataloader, tokenizer_src , tokenizer_tgt = get_ds(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "007a72ad-9fb8-476b-8f1b-dc8d26b0de4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.BilingualDataset object at 0x000002EC7EC941C0>\n"
     ]
    }
   ],
   "source": [
    "print(tds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f9140be8-262c-44e9-88aa-a49416bc0a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer(version=\"1.0\", truncation=None, padding=None, added_tokens=[{\"id\":0, \"content\":\"[UNK]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":1, \"content\":\"[PAD]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":2, \"content\":\"[SOS]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":3, \"content\":\"[EOS]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}], normalizer=None, pre_tokenizer=Whitespace(), post_processor=None, decoder=None, model=WordLevel(vocab={\"[UNK]\":0, \"[PAD]\":1, \"[SOS]\":2, \"[EOS]\":3, \",\":4, \"the\":5, \"and\":6, \".\":7, \"to\":8, \"I\":9, \"of\":10, \"a\":11, \"'\":12, \"in\":13, \"was\":14, \"that\":15, \"he\":16, \"it\":17, \";\":18, \"had\":19, \"his\":20, \"not\":21, \"with\":22, \"her\":23, \"you\":24, \"as\":25, \"for\":26, \"she\":27, \"my\":28, \"-\":29, \"at\":30, \"but\":31, \"him\":32, \"me\":33, \"is\":34, \"\"\":35, \"on\":36, \"be\":37, \":\":38, \"said\":39, \"have\":40, \"s\":41, \"all\":42, \"which\":43, \"so\":44, \"they\":45, \"by\":46, \"one\":47, \"were\":48, \"this\":49, \"them\":50, \"would\":51, \"from\":52, \",'\":53, \"or\":54, \"what\":55, \"up\":56, \"could\":57, \"!\":58, \"when\":59, \"He\":60, \"out\":61, \"The\":62, \"been\":63, \"there\":64, \"an\":65, \"are\":66, \"no\":67, \"who\":68, \"if\":69, \"we\":70, \"now\":71, \"about\":72, \"did\":73, \".\"\":74, \"very\":75, \".'\":76, \"will\":77, \"--\":78, \"their\":79, \"?\":80, \"do\":81, \"more\":82, \"t\":83, \"?'\":84, \"!'\":85, \"Levin\":86, \"But\":87, \"It\":88, \"into\":89, \"only\":90, \"And\":91, \"then\":92, \"some\":93, \"time\":94, \"your\":95, \"like\":96, \"go\":97, \"thought\":98, ...}, unk_token=\"[UNK]\"))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41d9f9ae-084e-4b5b-bf19-43d342741936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22463"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c673b83-af57-4156-9063-3bbd45d1c897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing the Training DataLoader...\n",
      "\n",
      "Batch 1:\n",
      "\n",
      "Key: encoder_input\n",
      "Shape: torch.Size([8, 350])\n",
      "Content (First Sample): tensor([    2,  4724,   878,   110,   260, 11501,   746,  3888, 11501,  9176,\n",
      "           13,     5, 14352,    10,   150,    43,   197,   809,     7,     3,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])\n",
      "\n",
      "Key: decoder_input\n",
      "Shape: torch.Size([8, 350])\n",
      "Content (First Sample): tensor([    2,  1403, 12631,  1200, 12377,     4,   481,   220, 14981,     4,\n",
      "            0,    68,  5437,     7,  2817,  6594,     4,     8,     0,    11,\n",
      "          248,     5,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])\n",
      "\n",
      "Key: encoder_mask\n",
      "Shape: torch.Size([8, 1, 1, 350])\n",
      "Content (First Sample): tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0]]], dtype=torch.int32)\n",
      "\n",
      "Key: decoder_mask\n",
      "Shape: torch.Size([8, 1, 350, 350])\n",
      "Content (First Sample): tensor([[[1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0]]], dtype=torch.int32)\n",
      "\n",
      "Key: label\n",
      "Shape: torch.Size([8, 350])\n",
      "Content (First Sample): tensor([ 1403, 12631,  1200, 12377,     4,   481,   220, 14981,     4,     0,\n",
      "           68,  5437,     7,  2817,  6594,     4,     8,     0,    11,   248,\n",
      "            5,     3,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])\n",
      "\n",
      "Key: src_text - Type: <class 'list'>\n",
      "\n",
      "Key: tgt_text - Type: <class 'list'>\n",
      "\n",
      "\n",
      "--- Text Reconstruction from Token IDs ---\n",
      "Source Text:\n",
      "Nobody knows how many rebellions besides political rebellions ferment in the masses of life which people earth.\n",
      "Reconstructed Source Tokens:\n",
      "Nobody knows how many rebellions besides political rebellions ferment in the masses of life which people earth .\n",
      "Target Text:\n",
      "Nessuno suppone quante rivolte, oltre quelle politiche, fermentino nella massa di esseri viventi, che popolano la terra.\n",
      "Reconstructed Target Tokens:\n",
      "Nessuno suppone quante rivolte , oltre quelle politiche , nella massa di esseri viventi , che la terra .\n",
      "\n",
      "Visualizing the Validation DataLoader...\n",
      "\n",
      "Batch 1:\n",
      "\n",
      "Key: encoder_input\n",
      "Shape: torch.Size([1, 350])\n",
      "Content (First Sample): tensor([   2,  337,    8,    5, 8667,    4,    9, 1071,    8,  725,   50,   93,\n",
      "         517,   52,  881,    4,   22,   11,  129, 2512,   10, 4575,    4,   69,\n",
      "          45,   51, 2976,  473,    8, 5234,  276,   43,    9,  502,   57,   21,\n",
      "        3398,    7,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1])\n",
      "\n",
      "Key: decoder_input\n",
      "Shape: torch.Size([1, 350])\n",
      "Content (First Sample): tensor([    2,  1255,   419,  9214,     4, 20565,     7, 21719,    69,   641,\n",
      "          526,   253,    10,  1006,    78,    15,   466,  1720,     7,  2572,\n",
      "            0,     4,    42,   661,   579,  1684,   105,    10,  6152,     4,\n",
      "         2185,     8,    16,   821,    12,   708,  1924,     5,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])\n",
      "\n",
      "Key: encoder_mask\n",
      "Shape: torch.Size([1, 1, 1, 350])\n",
      "Content (First Sample): tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0]]], dtype=torch.int32)\n",
      "\n",
      "Key: decoder_mask\n",
      "Shape: torch.Size([1, 1, 350, 350])\n",
      "Content (First Sample): tensor([[[1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0]]], dtype=torch.int32)\n",
      "\n",
      "Key: label\n",
      "Shape: torch.Size([1, 350])\n",
      "Content (First Sample): tensor([ 1255,   419,  9214,     4, 20565,     7, 21719,    69,   641,   526,\n",
      "          253,    10,  1006,    78,    15,   466,  1720,     7,  2572,     0,\n",
      "            4,    42,   661,   579,  1684,   105,    10,  6152,     4,  2185,\n",
      "            8,    16,   821,    12,   708,  1924,     5,     3,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])\n",
      "\n",
      "Key: src_text - Type: <class 'list'>\n",
      "\n",
      "Key: tgt_text - Type: <class 'list'>\n",
      "\n",
      "\n",
      "--- Text Reconstruction from Token IDs ---\n",
      "Source Text:\n",
      "As to the Englishmen, I promised to send them some women from England, with a good cargo of necessaries, if they would apply themselves to planting—which I afterwards could not perform.\n",
      "Reconstructed Source Tokens:\n",
      "As to the Englishmen , I promised to send them some women from England , with a good cargo of necessaries , if they would apply themselves to planting — which I afterwards could not perform .\n",
      "Target Text:\n",
      "Quanto agli Inglesi, promisi di spedir loro alcune donne dall’Inghilterra ed un buon carico di stromenti rurali, se avessero voluto darsi all’agricoltura, promessa che in appresso non potei mantenere.\n",
      "Reconstructed Target Tokens:\n",
      "Quanto agli Inglesi , promisi di spedir loro alcune donne dall ’ Inghilterra ed un buon carico di stromenti , se avessero voluto darsi all ’ agricoltura , promessa che in appresso non potei mantenere .\n"
     ]
    }
   ],
   "source": [
    "# Visualization of variables in the DataLoader\n",
    "def visualize_dataloader(train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt):\n",
    "    # Get a single batch from the train DataLoader\n",
    "    print(\"Visualizing the Training DataLoader...\\n\")\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        print(f\"Batch {batch_idx + 1}:\\n\")\n",
    "        \n",
    "        for key, value in batch.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                print(f\"Key: {key}\")\n",
    "                print(f\"Shape: {value.shape}\")\n",
    "                print(f\"Content (First Sample): {value[0]}\\n\")\n",
    "            else:\n",
    "                print(f\"Key: {key} - Type: {type(value)}\\n\")\n",
    "        \n",
    "        print(\"\\n--- Text Reconstruction from Token IDs ---\")\n",
    "        print(\"Source Text:\")\n",
    "        print(batch[\"src_text\"][0])  # Original source text\n",
    "        print(\"Reconstructed Source Tokens:\")\n",
    "        print(tokenizer_src.decode(batch[\"encoder_input\"][0].tolist()))\n",
    "        \n",
    "        print(\"Target Text:\")\n",
    "        print(batch[\"tgt_text\"][0])  # Original target text\n",
    "        print(\"Reconstructed Target Tokens:\")\n",
    "        print(tokenizer_tgt.decode(batch[\"decoder_input\"][0].tolist()))\n",
    "        \n",
    "        # Break after visualizing the first batch\n",
    "        break\n",
    "\n",
    "    print(\"\\nVisualizing the Validation DataLoader...\\n\")\n",
    "    for batch_idx, batch in enumerate(val_dataloader):\n",
    "        print(f\"Batch {batch_idx + 1}:\\n\")\n",
    "        \n",
    "        for key, value in batch.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                print(f\"Key: {key}\")\n",
    "                print(f\"Shape: {value.shape}\")\n",
    "                print(f\"Content (First Sample): {value[0]}\\n\")\n",
    "            else:\n",
    "                print(f\"Key: {key} - Type: {type(value)}\\n\")\n",
    "        \n",
    "        print(\"\\n--- Text Reconstruction from Token IDs ---\")\n",
    "        print(\"Source Text:\")\n",
    "        print(batch[\"src_text\"][0])  # Original source text\n",
    "        print(\"Reconstructed Source Tokens:\")\n",
    "        print(tokenizer_src.decode(batch[\"encoder_input\"][0].tolist()))\n",
    "        \n",
    "        print(\"Target Text:\")\n",
    "        print(batch[\"tgt_text\"][0])  # Original target text\n",
    "        print(\"Reconstructed Target Tokens:\")\n",
    "        print(tokenizer_tgt.decode(batch[\"decoder_input\"][0].tolist()))\n",
    "        \n",
    "        # Break after visualizing the first batch\n",
    "        break\n",
    "\n",
    "# Call the visualization function\n",
    "visualize_dataloader(train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f0d21f-18c2-426f-86ec-ed167a0615c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_39_env",
   "language": "python",
   "name": "pytorch_39_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
