{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92ba7b85-d629-41b3-8db6-caddd9e95d6e",
   "metadata": {},
   "source": [
    "# Coding the Real Transformer from Scratch \n",
    "*(Understand the basic 1 encoder and 1 decoder without masking transformer FIRST.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7c2b9c-129b-47a8-b8da-543a2af22132",
   "metadata": {},
   "source": [
    "<img src=\"transformer images/transformer_model.PNG\" alt=\"Sample Image\" style=\"width:1000px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925ec349-1195-4486-88ad-10bd286ee3a3",
   "metadata": {},
   "source": [
    "# Input Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788e0f6e-d267-4fa2-a5e6-15c1ba9d2fa0",
   "metadata": {},
   "source": [
    "<img src=\"transformer images/embedding.PNG\" alt=\"Sample Image\" style=\"width:1000px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc5c477-62fe-45f4-af31-2fdefd1a923f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598ef3cb-56b3-423a-a6ea-1d9bd7e29724",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None: # d_model : dimensions , vocab_size : How many words are there in the vocabulary.\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len) --> (batch, seq_len, d_model)\n",
    "        # Multiply by sqrt(d_model) to scale the embeddings according to the paper\n",
    "        return self.embedding(x) * math.sqrt(self.d_model) # This is in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594e9260-563d-4b94-9a49-cdf415171cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b31a653-3cf2-4126-bbe3-1dd2dbc6ead5",
   "metadata": {},
   "source": [
    "```python\n",
    "vocab = {\"hello\": 0, \",\": 1, \"there\": 2}\n",
    "sent = \"hello , there\"  # Input sentence\n",
    "tokenized = [vocab[word] for word in sent.split()]  # Convert words to indices\n",
    "input_tensor = torch.tensor(tokenized)\n",
    "ie = InputEmbeddings(d_model=512, vocab_size=len(vocab))\n",
    "output = ie.forward(input_tensor)\n",
    "print(output.shape)\n",
    "```\n",
    "You can run this to see for yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780d41ac-e581-48ba-a48e-c7a6d4f59260",
   "metadata": {},
   "source": [
    "# Positional Encoding\n",
    "<img src=\"transformer images/positional_encoding.PNG\" alt=\"Sample Image\" style=\"width:1000px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2134d75-9a95-4d25-a04f-8b251bfc7335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_model : We are creating another similar vector of size same as before.\n",
    "# seq_len : Maximum length for the SENTENC, BECAUSE WE NEED TO CREATE 1 VECTOR FOR EACH POSITION AS IN PIC ABOVE.\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # First we will build a matrix of seq_len,d_model. Because we need 512 features for each token. and total token is seq_len.\n",
    "        # Create a matrix of shape (seq_len, d_model)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        # Create a vector of shape (seq_len)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
    "        # Create a vector of shape (d_model)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
    "        # Add a batch dimension to the positional encoding\n",
    "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
    "        # Register the positional encoding as a buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dbac39-f48b-4e36-9613-b72ddb56ebd1",
   "metadata": {},
   "source": [
    "# Layer Normalization\n",
    "<img src=\"transformer images/ln.PNG\" alt=\"Sample Image\" style=\"width:1000px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a5b872-6b34-4c3b-8caf-6791af2a527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, eps:float=10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps ## Here eps is important because, if the denomitor is 0 , the the x will be undefined or very huge.\n",
    "        self.alpha = nn.Parameter(torch.ones(features)) # alpha is a learnable parameter\n",
    "        self.bias = nn.Parameter(torch.zeros(features)) # bias is a learnable parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, hidden_size)\n",
    "         # Keep the dimension for broadcasting\n",
    "        mean = x.mean(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "        # Keep the dimension for broadcasting\n",
    "        std = x.std(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "        # eps is to prevent dividing by zero or when std is very small\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e1c30d-b3a9-4c65-a440-e9dd1b0d3a6b",
   "metadata": {},
   "source": [
    "# Feed Forward Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3058f8-d744-4e2f-9a70-240df262012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f26e844-765f-43f6-8aa6-6ec6f21d19af",
   "metadata": {},
   "source": [
    "# Multihead Attention\n",
    "<img src=\"transformer images/Multiatt.PNG\" alt=\"Sample Image\" style=\"width:1000px;\">\n",
    "\n",
    "## Points to be noted. \n",
    "- We need to divide the d_model with number of head.\n",
    "- self.d_k : See the picture above , the small matrices are dk , this means , if we are using multi head attention then each head will see the whole sentence but different part of the dimension from 512. Basically it is the size of that matrix.\n",
    "- If we want some words to not to interact with other words , we mask them.\n",
    "- As you can see , the formula for attention , we sqrt(dk) there , that means we are calculating it for head.\n",
    "- So if we want some words to not to interact with others , we set them a very small value , so that after softmax , it will be nearly zero , and the model will not focus on those much.\n",
    "\n",
    "### Arguments:\n",
    "\n",
    "- d_model: Dimensionality of the input embeddings. This is the size of the input vectors.\n",
    "- h: Number of attention heads. Each head processes a subset of the input embedding.\n",
    "- dropout: Probability for dropout, used to prevent overfitting.\n",
    "\n",
    "### Importance of d_k\n",
    "\n",
    "- Multi-head attention divides the d_model dimension into h smaller chunks (d_k).\n",
    "- The embeddings are split across the heads, allowing the model to focus on multiple parts of the sequence in parallel.\n",
    "- The size of each attention head's embedding (d_model / h).\n",
    "- Smaller chunks of embeddings are easier to compute and allow independent attention mechanisms in each head.\n",
    "\n",
    "### w_o importance\n",
    "- w_o: This merges the attention outputs from all heads back into the original embedding space.\n",
    "\n",
    "### Attention Scores\n",
    "- query @ key.transpose(-2, -1):\n",
    "  - Performs scaled dot-product attention.\n",
    "  - query: (batch, h, seq_len, d_k)\n",
    "  - key.transpose(-2, -1): (batch, h, d_k, seq_len)\n",
    "  - Output: (batch, h, seq_len, seq_len).\n",
    "- Division by sqrt(d_k):\n",
    "  - Scales the scores to stabilize gradients (avoids very large/small values).\n",
    "\n",
    "### Why Masking?\n",
    "- Prevents the model from attending to padding tokens in the sequence by setting their scores to a very low value (-1e9).\n",
    "\n",
    "###  Reshaping and Transposing\n",
    "\n",
    "``` query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)```\n",
    "- Reshaping:\n",
    "  - Splits the last dimension (d_model) into h heads and d_k dimensions: (batch, seq_len, d_model) â†’ (batch, seq_len, h, d_k).\n",
    "- Transposing:\n",
    "  - Brings h to the second dimension for computation: (batch, seq_len, h, d_k) â†’ (batch, h, seq_len, d_k).\n",
    " \n",
    "### Restoring Shape\n",
    "\n",
    "```x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)```\n",
    "- Transpose Back:\n",
    "  - Brings the h dimension back to its original position.\n",
    "- Contiguous and View:\n",
    "  - Ensures the tensor is in memory order and reshaped to (batch, seq_len, d_model).\n",
    " \n",
    "### Final Linear Transformation\n",
    "\n",
    "```return self.w_o(x)```\n",
    "- Purpose:\n",
    "  - Combines the attention outputs of all heads back into the original embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4c3915-d5a0-40e4-b82b-fa9fe4fa761c",
   "metadata": {},
   "source": [
    "## Understanding the Dimension Manipulation over here.\n",
    "#### Linear Projection (nn.Linear)\n",
    "Each of q, k, v passes through a learnable linear layer to project the input embeddings into new spaces:\n",
    "\n",
    "```python\n",
    "query = self.w_q(q)  # Shape: (Batch, Seq_Len, d_model)\n",
    "key   = self.w_k(k)  # Shape: (Batch, Seq_Len, d_model)\n",
    "value = self.w_v(v)  # Shape: (Batch, Seq_Len, d_model)\n",
    "```\n",
    "\n",
    "\n",
    "- Significance:\n",
    "  - The Linear layer ensures that the embeddings can be transformed into different spaces for the Query, Key, and Value. This is necessary because their roles in attention computation differ.\n",
    " \n",
    "#### Reshape Using view: Splitting Heads\n",
    "- The next operation reshapes the tensor to separate the d_model dimension into multiple heads (h), each of size d_k:\n",
    "\n",
    "```python\n",
    "query = query.view(Batch, Seq_Len, h, d_k)\n",
    "key   = key.view(Batch, Seq_Len, h, d_k)\n",
    "value = value.view(Batch, Seq_Len, h, d_k)\n",
    "```\n",
    "\n",
    "\n",
    "```Input Shape: (Batch, Seq_Len, d_model)```\n",
    "\n",
    "```Output Shape: (Batch, Seq_Len, h, d_k)```\n",
    "\n",
    "#### Why Split Heads?\n",
    "\n",
    "- Multi-head attention allows multiple subspaces (heads) to attend to different parts of the sequence independently.\n",
    "d_k = d_model / h: Each head operates on a smaller embedding space.\n",
    "Example: For Batch=2, Seq_Len=4, d_model=12, h=3:\n",
    "\n",
    "```Before view: (2, 4, 12)```\n",
    "\n",
    "```After view: (2, 4, 3, 4) (3 heads, each with 4 dimensions).```\n",
    "\n",
    "#### Transpose: Rearrange Dimensions for Attention\n",
    "The tensor is then transposed to make the h (heads) dimension adjacent to the Batch dimension:\n",
    "\n",
    "```python\n",
    "query = query.transpose(1, 2)  # Shape: (Batch, Seq_Len, h, d_k) â†’ (Batch, h, Seq_Len, d_k)\n",
    "key   = key.transpose(1, 2)    # Same transformation\n",
    "value = value.transpose(1, 2)  # Same transformation\n",
    "```\n",
    "\n",
    "```Input Shape: (Batch, Seq_Len, h, d_k)```\n",
    "\n",
    "```Output Shape: (Batch, h, Seq_Len, d_k)```\n",
    "\n",
    "#### Why Transpose?\n",
    "\n",
    "- Attention is calculated independently for each head.\n",
    "- By moving h next to Batch, we can easily parallelize computations for all heads.\n",
    "```Example: For Batch=2, Seq_Len=4, h=3, d_k=4:```\n",
    "\n",
    "```Before transpose: (2, 4, 3, 4)```\n",
    "\n",
    "```After transpose: (2, 3, 4, 4) (Batch=2, Heads=3, Seq_Len=4, d_k=4).```\n",
    "\n",
    "#### Intuition Behind Dimensions and Their Role\n",
    "- Batch and h adjacency: Each batch contains multiple sequences. Each sequence is split into h heads, so this adjacency allows us to calculate attention across all heads in one step.\n",
    "- Seq_Len and d_k: After transposition, these remain the dimensions of focus for each head. Each head processes the same sequence length (Seq_Len) but focuses on its respective subspace of features (d_k).\n",
    "#### Flow of Computation\n",
    "- Preparation: The input embeddings are split into h heads using a linear layer, producing (Batch, Seq_Len, h, d_k).\n",
    "\n",
    "- Transpose: Rearrange dimensions with .transpose(1, 2) to get (Batch, h, Seq_Len, d_k). Now:\n",
    "\n",
    "  - Each headâ€™s attention operates on its slice (Seq_Len, d_k).\n",
    "  - Computation for all heads is parallelized because Batch and h are adjacent.\n",
    "- Attention Calculation: Compute attention per head:\n",
    "\n",
    "- *ð‘„ð¾âŠ¤* becomes ```(Batch, h, Seq_Len, Seq_Len).```\n",
    "- Multiply with V, producing (Batch, h, Seq_Len, d_k).\n",
    "#### Reconstruction: \n",
    "- Concatenate the heads back after attention computations and project the combined output back to the original embedding space.\n",
    "\n",
    "#### Why Does Moving h Next to Batch Help?\n",
    "- Placing h next to Batch ensures all heads are processed simultaneously by modern hardware (e.g., GPUs/TPUs).\n",
    "- The resulting shape (Batch, h, Seq_Len, d_k) simplifies tensor operations because heads are now treated as additional batches for computation purposes.\n",
    "\n",
    "#### Why self.w_o?\n",
    "Now, the self.w_o layer is needed because:\n",
    "\n",
    "- **Projection into the desired output space:** Even though the output tensor x has the correct shape (Batch, Seq_Len, d_model), it doesn't necessarily have the correct content yet.\n",
    "  - The purpose of self.w_o is to project the concatenated and attended outputs of all heads back into the desired feature space.\n",
    "  - This is important because, during the attention process, each head computes its own set of attention scores, and the resulting tensor might not represent the desired feature mapping.\n",
    "  - self.w_o ensures that the output is appropriately transformed back into the space of d_model features that the model expects after the attention step.\n",
    "- **Learnable transformation:** self.w_o introduces learnable parameters that allow the model to learn how to combine the results from different heads and project them into the appropriate feature space.\n",
    "  - The learned weights in self.w_o allow the model to mix and combine information from all attention heads more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b712eb-0f8f-40f2-bc65-7807a8428fbd",
   "metadata": {},
   "source": [
    "## Understanding the Formula: Example with Simple Values\r\n",
    "\r\n",
    "Letâ€™s work through a simple example to see how the attention formula works.\r\n",
    "\r\n",
    "### Input Sentence:\r\n",
    "Consider the sentence \"The cat sleeps.\" (This is a simplified example with 3 words).\r\n",
    "\r\n",
    "Letâ€™s assume that each word has been embedded into a vector of size \\( d_k = 3 \\), and we'll use toy values for simplicity.\r\n",
    "\r\n",
    "### Step 1: Construct Query, Key, and Value Matrices\r\n",
    "\r\n",
    "For simplicity, letâ€™s assume:\r\n",
    "\r\n",
    "\\[\r\n",
    "Q =\r\n",
    "\\begin{bmatrix}\r\n",
    "1 & 0 & 0 \\\\\r\n",
    "0 & 1 & 0 \\\\\r\n",
    "0 & 0 & 1\r\n",
    "\\end{bmatrix}, \r\n",
    "\\quad \r\n",
    "K =\r\n",
    "\\begin{bmatrix}\r\n",
    "1 & 1 & 0 \\\\\r\n",
    "0 & 1 & 1 \\\\\r\n",
    "1 & 0 & 1\r\n",
    "\\end{bmatrix}, \r\n",
    "\\quad \r\n",
    "V =\r\n",
    "\\begin{bmatrix}\r\n",
    "0 & 1 & 1 \\\\\r\n",
    "1 & 0 & 1 \\\\\r\n",
    "1 & 1 & 0\r\n",
    "\\end{bmatrix}\r\n",
    "\\]\r\n",
    "\r\n",
    "Each row in \\( Q \\), \\( K \\), and \\( V \\) corresponds to a vector representation of the words \"The\", \"cat\", and \"sleeps\", respectively.\r\n",
    "\r\n",
    "### Step 2: Compute the Dot Product \\( QK^T \\)\r\n",
    "\r\n",
    "Now, we compute the dot product between \\( Q \\) and \\( K^T \\):\r\n",
    "\r\n",
    "\\[\r\n",
    "QK^T =\r\n",
    "\\begin{bmatrix}\r\n",
    "1 & 0 & 0 \\\\\r\n",
    "0 & 1 & 0 \\\\\r\n",
    "0 & 0 & 1\r\n",
    "\\end{bmatrix}\r\n",
    "\\begin{bmatrix}\r\n",
    "1 & 1 & 0 \\\\\r\n",
    "0 & 1 & 1 \\\\\r\n",
    "1 & 0 & 1\r\n",
    "\\end{bmatrix}\r\n",
    "=\r\n",
    "\\begin{bmatrix}\r\n",
    "1 & 0 & 1 \\\\\r\n",
    "0 & 1 & 0 \\\\\r\n",
    "1 & 0 & 1\r\n",
    "\\end{bmatrix}\r\n",
    "\\]\r\n",
    "\r\n",
    "This gives us a matrix that indicates how much attention each word should pay to each other word. For example, the first row \\( [1, 0, 1] \\) means the word \"The\" pays attention to \"The\" and \"sleeps\", but not to \"cat\".\r\n",
    "\r\n",
    "### Step 3: Scale by \\( d_k \\)\r\n",
    "\r\n",
    "For \\( d_k = 3 \\), we scale the dot product by \\( \\sqrt{d_k} \\):\r\n",
    "\r\n",
    "\\[\r\n",
    "QK^T = \r\n",
    "\\begin{bmatrix}\r\n",
    "\\frac{1}{\\sqrt{3}} & 0 & \\frac{1}{\\sqrt{3}} \\\\\r\n",
    "0 & \\frac{1}{\\sqrt{3}} & 0 \\\\\r\n",
    "\\frac{1}{\\sqrt{3}} & 0 & \\frac{1}{\\sqrt{3}}\r\n",
    "\\end{bmatrix}\r\n",
    "\\]\r\n",
    "\r\n",
    "### Step 4: Apply Softmax\r\n",
    "\r\n",
    "Next, we apply the softmax to each row of the matrix to normalize the values. \r\n",
    "\r\n",
    "\\[\r\n",
    "\\text{Softmax}(QK^T) = \r\n",
    "\\text{Softmax} \\left( \r\n",
    "\\begin{bmatrix}\r\n",
    "\\frac{1}{\\sqrt{3}} & 0 & \\frac{1}{\\sqrt{3}} \\\\\r\n",
    "0 & \\frac{1}{\\sqrt{3}} & 0 \\\\\r\n",
    "\\frac{1}{\\sqrt{3}} & 0 & \\frac{1}{\\sqrt{3}}\r\n",
    "\\end{bmatrix}\r\n",
    "\\right)\r\n",
    "=\r\n",
    "\\begin{bmatrix}\r\n",
    "0.5 & 0.25 & 0.25 \\\\\r\n",
    "0.25 & 0.5 & 0.25 \\\\\r\n",
    "0.5 & 0.25 & 0.25\r\n",
    "\\end{bmatrix}\r\n",
    "\\]\r\n",
    "\r\n",
    "This gives the normalized attention weights for each word in the sequence, indicating how much focus each word should pay to others.\r\n",
    "\r\n",
    "### Step 5: Weighted Sum with Value Matrix\r\n",
    "\r\n",
    "Now, we multiply the attention scores with the Value matrix \\( V \\):\r\n",
    "\r\n",
    "\\[\r\n",
    "\\text{Attention Output} = \r\n",
    "\\text{Softmax}(QK^T) \\times V =\r\n",
    "\\begin{bmatrix}\r\n",
    "0.5 & 0.25 & 0.25 \\\\\r\n",
    "0.25 & 0.5 & 0.25 \\\\\r\n",
    "0.5 & 0.25 & 0.25\r\n",
    "\\end{bmatrix}\r\n",
    "\\begin{bmatrix}\r\n",
    "0 & 1 & 1 \\\\\r\n",
    "1 & 0 & 1 \\\\\r\n",
    "1 & 1 & 0\r\n",
    "\\end{bmatrix}\r\n",
    "\\]\r\n",
    "\r\n",
    "The resulting matrix is:\r\n",
    "\r\n",
    "\\[\r\n",
    "\\text{Attention Output} =\r\n",
    "\\begin{bmatrix}\r\n",
    "0.5 & 0.75 & 0.5 \\\\\r\n",
    "0.25 & 0.25 & 0.5 \\\\\r\n",
    "0.5 & 0.75 & 0.5\r\n",
    "\\end{bmatrix}\r\n",
    "\\]\r\n",
    "\r\n",
    "### Result:\r\n",
    "\r\n",
    "The output of this step is a new set of vectors that are influenced by the attention scores. These vectors are now context-aware because each wordâ€™s representation has been adjusted based on its relationship to other words in the sequence. For example, the vector for \"The\" has been adjusted based on its relationship to \"sleeps\" and \"cat\".\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c927f9-b34a-430c-a318-13231e0d375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # Embedding vector size\n",
    "        self.h = h # Number of heads\n",
    "        # Make sure d_model is divisible by h\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "\n",
    "        self.d_k = d_model // h # Dimension of vector seen by each head\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]\n",
    "        # Just apply the formula from the paper\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
    "        # return attention scores which can be used for visualization\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k) \n",
    "        #  batch,seq_len,d_model --> batch,seq_len,h,d_k --> batch , h , seq_len, d_k , This means each head will see the whole sentence but a smaller part of the embedding.\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Calculate attention\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "        \n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        # Multiply by Wo\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)  \n",
    "        return self.w_o(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8078b80a-1194-4bb4-b83b-87784b8243fa",
   "metadata": {},
   "source": [
    "# Residual Connection\n",
    "Here the sublayer can be the multi head attention layer. Check the diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874595b0-e9e3-46d2-9912-231e91345c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    \n",
    "        def __init__(self, features: int, dropout: float) -> None:\n",
    "            super().__init__()\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.norm = LayerNormalization(features)\n",
    "    \n",
    "        def forward(self, x, sublayer):\n",
    "            return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b04a693-4d8b-4df1-992a-b02f4b98e4c2",
   "metadata": {},
   "source": [
    "# Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e79118-5845-4718-a369-6a2038400e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask): # we use mask in encoding, because we want to hide the pad tokens.\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7fe693-c700-4f02-a35a-5ae013923ad2",
   "metadata": {},
   "source": [
    "# Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703e273f-1eab-48a7-a57e-5dc925bfb601",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a5d006-bbec-4b70-bd74-bb22910acd77",
   "metadata": {},
   "source": [
    "# Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f6ab89-c4fb-4c30-9483-8c313a59ab7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x) -> None:\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c91e12-6b68-440a-93ba-c9c7fcfe7cd0",
   "metadata": {},
   "source": [
    "# Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fa4d73-09a9-4485-9cc0-e39e46a29f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        # (batch, seq_len, d_model)\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
    "        # (batch, seq_len, d_model)\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "    \n",
    "    def project(self, x):\n",
    "        # (batch, seq_len, vocab_size)\n",
    "        return self.projection_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31daf8f-70f9-4e56-9a87-f25e0cfd15a7",
   "metadata": {},
   "source": [
    "# Build Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439fd530-99eb-400a-8d27-a8d3a4992c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int=512, N: int=1, h: int=1, dropout: float=0.1, d_ff: int=512) -> Transformer:\n",
    "    # Create the embedding layers\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "\n",
    "    # Create the positional encoding layers\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
    "    \n",
    "    # Create the encoder blocks\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "    # Create the decoder blocks\n",
    "    decoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "    \n",
    "    # Create the encoder and decoder\n",
    "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
    "    \n",
    "    # Create the projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "    \n",
    "    # Create the transformer\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "    \n",
    "    # Initialize the parameters\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a150221-eba0-48d7-8eb1-6ee5c977dcd0",
   "metadata": {},
   "source": [
    "# TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc182560-bc70-4bbe-86f9-6a3d2333386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader,random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9d209a-e62b-426c-a644-4bafcd8df4f0",
   "metadata": {},
   "source": [
    "## Build Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668bc177-3099-499f-a259-aa60936aee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(ds,lang):\n",
    "    for item in ds:\n",
    "        yield item['translation'][lang]\n",
    "\n",
    "def get_or_build_tokenizer(config,ds,lang):\n",
    "    # config['tokenizer_file'] = '..tokenizers/tokenizer_{0}.json'\n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token='[UNK]'))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\",\"[PAD]\",\"[SOS]\",\"[EOS]\"], min_frequency=2) # min_frequency A word to have it in our vocab , the frequency of that word must be 2.\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds,lang),trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2667dc2-6ffd-4834-8c2a-7300e370e5d9",
   "metadata": {},
   "source": [
    "## Load the dataset , build the tokenizer and Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df94dd46-7ebb-4cbd-af7f-4be2bea0cb06",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### Overview of BilingualDataset Class:\n",
    "This is a custom Dataset class used for preparing a bilingual dataset for sequence-to-sequence tasks, such as machine translation. It takes pairs of source and target texts, tokenizes them, and prepares them in the required format for feeding into a model.\n",
    "\n",
    "```python\n",
    "def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
    "    super().__init__()\n",
    "    self.ds = ds\n",
    "    self.tokenizer_src = tokenizer_src\n",
    "    self.tokenizer_tgt = tokenizer_tgt\n",
    "    self.src_lang = src_lang\n",
    "    self.tgt_lang = tgt_lang\n",
    "\n",
    "    self.sos_token = torch.Tensor([tokenizer_src.token_to_id(['SOS'])], dtype=torch.int64)  # SOS token for source\n",
    "    self.eos_token = torch.Tensor([tokenizer_src.token_to_id(['EOS'])], dtype=torch.int64)  # EOS token for source\n",
    "    self.pad_token = torch.Tensor([tokenizer_src.token_to_id(['PAD'])], dtype=torch.int64)  # PAD token for source\n",
    "```\n",
    "#### Purpose of __init__: \n",
    "- The constructor initializes the dataset and tokenizer for both the source and target languages, and sets up the special tokens (SOS, EOS, PAD) for the source language using the source tokenizer.\n",
    "- sos_token, eos_token, and pad_token are tensors holding the indices of the corresponding tokens in the tokenizer's vocabulary.\n",
    "\n",
    "```python\n",
    "def __len__(self):\n",
    "    return len(self.ds)\n",
    "```\n",
    "\n",
    "\n",
    "#### Purpose of __len__: \n",
    "- Returns the number of data samples in the dataset, which is simply the length of self.ds. This is needed for PyTorch to know the total number of items in the dataset when iterating.\n",
    "\n",
    "```python\n",
    "def __getitem__(self, index):\n",
    "    src_target_pair = self.ds[index]\n",
    "    src_text = src_target_pair['translation'][self.src_lang]\n",
    "    tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
    "```\n",
    "\n",
    "#### Purpose of getitem: \n",
    "- This function retrieves a source-target pair from the dataset self.ds at the given index.\n",
    "- src_text and tgt_text are the actual source and target sentences based on the src_lang and tgt_lang keys in the dataset.\n",
    "\n",
    "#### Tokenization and Padding Calculation:\n",
    "```python\n",
    "enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2  # Adding SOS and EOS to the source\n",
    "dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1  # Adding only SOS to the target\n",
    "```\n",
    "\n",
    "#### Purpose of padding calculation: \n",
    "- The sentences (src_text and tgt_text) are tokenized into numerical indices using the source and target tokenizers (tokenizer_src and tokenizer_tgt).\n",
    "- Padding tokens are calculated to ensure the source and target sequences are of the desired length seq_len. We subtract 2 for source padding (SOS and EOS), and 1 for target padding (SOS only, since we stop predicting once EOS is generated).\n",
    "\n",
    "#### Padding and Token Concatenation:\n",
    "```python\n",
    "if enc_num_padding_tokens < 0 or doc_num_padding_tokens < 0:\n",
    "    raise ValueError('Sentence is too long')\n",
    "\n",
    "# Add SOS and EOS to the source text\n",
    "encoder_input = torch.cat([\n",
    "    self.sos_token,\n",
    "    torch.Tensor(enc_input_tokens, dtype=torch.int64),\n",
    "    self.eos_token,\n",
    "    torch.Tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64)\n",
    "])\n",
    "\n",
    "# Add SOS to the target text\n",
    "decoder_input = torch.cat([\n",
    "    self.sos_token,\n",
    "    torch.Tensor(dec_input_tokens, dtype=torch.int64),\n",
    "    torch.Tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)\n",
    "])\n",
    "\n",
    "# Add EOS to the label text\n",
    "label = torch.cat([\n",
    "    torch.Tensor(dec_input_tokens, dtype=torch.int64),\n",
    "    self.pad_token,\n",
    "    torch.Tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)\n",
    "])\n",
    "```\n",
    "\n",
    "Purpose of cocatenation:\n",
    "- encoder_input: The source sequence is concatenated with the SOS token at the start, the EOS token at the end, and padding tokens (if necessary).\n",
    "- decoder_input: The target sequence is concatenated with the SOS token at the start and padding tokens at the end.\n",
    "- label: The target sequence (without SOS) is followed by the EOS token and then padded. This represents the true target output for the decoder.\n",
    "\n",
    "#### Assertion to Check Lengths:\n",
    "```python\n",
    "assert encoder_input.size(0) == self.seq_len\n",
    "assert decoder_input.size(0) == self.seq_len\n",
    "assert label.size(0) == self.seq_len\n",
    "```\n",
    "\n",
    "- Purpose of assertion: Ensures that the length of encoder_input, decoder_input, and label matches the desired seq_len.\n",
    "\n",
    "#### Masks and Returning the Data:\n",
    "```python\n",
    "return {\n",
    "    \"encoder_input\": encoder_input,  # (seq_len)\n",
    "    \"decoder_input\": decoder_input,  # (seq_len)\n",
    "    \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).int(),  # (1, 1, seq_len)\n",
    "    \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & casual_mask(decoder_input.size(0)),  # (1, seq_len) & (1, seq_len, seq_len)\n",
    "    \"label\": label,\n",
    "    \"src_text\": src_text,\n",
    "    \"target_text\": tgt_text\n",
    "}\n",
    "```\n",
    "\n",
    "#### Purpose of mask and returning data: \n",
    "- Returns a dictionary containing:\n",
    "- encoder_input, decoder_input, and label: The tokenized and padded inputs and target sequences.\n",
    "- encoder_mask: A mask for the encoder, where padding tokens are marked as 0 and non-padding tokens as 1.\n",
    "- decoder_mask: A mask for the decoder, including the causal mask (explained below).\n",
    "- src_text, target_text: The original source and target text sentences.\n",
    "- Causal Mask:\n",
    "```python\n",
    "def casual_mask(size):\n",
    "    mask = torch.triu(torch.ones(1, size, size), diagonal=1).type(torch.int)\n",
    "    return mask == 0\n",
    "```\n",
    "#### Purpose of casual mask:\n",
    "- This function creates a causal mask used in the decoder to prevent attending to future tokens during training (as per the \"autoregressive\" nature of the decoder). It ensures that each position in the decoder can only attend to itself and previous positions, but not to future tokens.\n",
    "- The torch.triu() function creates an upper triangular matrix, and mask == 0 ensures that only the lower triangular part is True (valid positions), while the rest are False (invalid positions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9427bb8-fea5-4f7d-ace5-b9548faf2b5c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class BilingualDataset(Dataset):\n",
    "\n",
    "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "\n",
    "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_target_pair = self.ds[idx]\n",
    "        src_text = src_target_pair['translation'][self.src_lang]\n",
    "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
    "\n",
    "        # Transform the text into tokens\n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "        # Add sos, eos and padding to each sentence\n",
    "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2  # We will add <s> and </s>\n",
    "        # We will only add <s>, and </s> only on the label\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1 # we will add only <s> at the start. So to skip that we did -1\n",
    "\n",
    "        # Make sure the number of padding tokens is not negative. If it is, the sentence is too long\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError(\"Sentence is too long\")\n",
    "\n",
    "        # Add <s> and </s> token\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Add only <s> token\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Add only </s> token\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Double check the size of the tensors to make sure they are all seq_len long\n",
    "        assert encoder_input.size(0) == self.seq_len\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert label.size(0) == self.seq_len\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input,  # (seq_len)\n",
    "            \"decoder_input\": decoder_input,  # (seq_len)\n",
    "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)\n",
    "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # (1, seq_len) & (1, seq_len, seq_len),\n",
    "            \"label\": label,  # (seq_len)\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text,\n",
    "        }\n",
    "\n",
    "def causal_mask(size):\n",
    "    mask = torch.triu(torch.ones(1,size,size),diagonal=1).type(torch.int)\n",
    "    return mask == 0\n",
    "\n",
    "\n",
    "\n",
    "def get_ds(config):\n",
    "    ds_raw = load_dataset('opus_books',f\"{config['lang_src']}-{config['lang_tgt']}\",split='train')\n",
    "\n",
    "    # build the tokenizer\n",
    "    tokenizer_src = get_or_build_tokenizer(config,ds_raw,config['lang_src'])\n",
    "    tokenizer_tgt = get_or_build_tokenizer(config,ds_raw,config['lang_tgt'])\n",
    "\n",
    "    # keep 90% for training and 10% for validation\n",
    "    train_ds_size = int(0.9 * len(ds_raw))\n",
    "    val_ds_size = len(ds_raw) - train_ds_size\n",
    "    train_ds_raw , val_ds_raw = random_split(ds_raw,[train_ds_size,val_ds_size])\n",
    "\n",
    "    # train_ds and val_ds\n",
    "    train_ds = BilingualDataset(train_ds_raw,tokenizer_src,tokenizer_tgt,config['lang_src'],config['lang_tgt'],config['seq_len'])\n",
    "    val_ds  = BilingualDataset(val_ds_raw,tokenizer_src,tokenizer_tgt,config['lang_src'],config['lang_tgt'],config['seq_len'])\n",
    "\n",
    "    max_len_src = 0\n",
    "    max_len_tgt = 0\n",
    "    for item in ds_raw:\n",
    "        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
    "        tgt_ids = tokenizer_src.encode(item['translation'][config['lang_tgt']]).ids\n",
    "        max_len_src = max(max_len_src , len(src_ids))\n",
    "        max_len_tgt = max(max_len_tgt,len(tgt_ids))\n",
    "    print(f\"Max length of Soure sentence : {max_len_src}\")\n",
    "    print(f\"Max length of target sentence : {max_len_tgt}\")\n",
    "\n",
    "    train_dataloader = DataLoader(train_ds,batch_size=config['batch_size'],shuffle= True)\n",
    "    val_dataloader = DataLoader(val_ds, batch_size = 1 , shuffle = True)\n",
    "\n",
    "    return train_dataloader , val_dataloader, tokenizer_src , tokenizer_tgt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb233829-0feb-4c10-9058-af2531bda99e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%writefile config.py\n",
    "from pathlib import Path\n",
    "def get_config():\n",
    "    return {\n",
    "        \"batch_size\" : 8,\n",
    "        \"num_epochs\" : 20,\n",
    "        \"lr\" : 10**-4,\n",
    "        \"seq_len\" : 350,\n",
    "        \"d_model\" : 512,\n",
    "        \"datasource\": 'opus_books',\n",
    "        \"lang_src\" : \"en\",\n",
    "        \"lang_tgt\" : \"it\",\n",
    "        \"model_folder\" : \"weights\",\n",
    "        \"model_basename\" : \"tmodel_\",\n",
    "        \"preload\" : None,\n",
    "        \"tokenizer_file\" : \"tokenizer_{0}.json\",\n",
    "        \"experiment_name\" : \"runs/tmodel\"\n",
    "    }\n",
    "def get_weights_file_path(config,epoch:str):\n",
    "    model_folder = config['model_folder']\n",
    "    model_basename = config['model_basename']\n",
    "    model_filename = f\"{model_basename}{epoch}.pt\"\n",
    "    return str(Path('.') / model_folder / model_filename)\n",
    "def latest_weights_file_path(config):\n",
    "    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n",
    "    model_filename = f\"{config['model_basename']}*\"\n",
    "    weights_files = list(Path(model_folder).glob(model_filename))\n",
    "    if len(weights_files) == 0:\n",
    "        return None\n",
    "    weights_files.sort()\n",
    "    return str(weights_files[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506de042-43eb-4bca-9916-89b91a3de839",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fb9ac8-15db-4b7e-9ae0-7eb23e3cb39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(config,vocab_src_len,vocab_tgt_len):\n",
    "    model = build_transformer(vocab_src_len,vocab_tgt_len,config['seq_len'],config['seq_len'],config['d_model'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4077f3-c7f6-459f-8de0-cd124c4bdca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from config import get_config,get_weights_file_path\n",
    "\n",
    "def train_model(config):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using Device {device}\")\n",
    "\n",
    "    # Create the model Folder\n",
    "    Path(config['model_folder']).mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "    # dataset \n",
    "    train_dataloader , val_dataloader, tokenizer_src , tokenizer_tgt = get_ds(config)\n",
    "    \n",
    "    # model\n",
    "    model = get_model(config,tokenizer_src.get_vocab_size() , tokenizer_tgt.get_vocab_size()).to(device)\n",
    "\n",
    "    # TensorBoard\n",
    "    writer = SummaryWriter(config['experiment_name'])\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr = config['lr'],eps=1e-9)\n",
    "\n",
    "    # Premodel loading\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "    if config['preload']:\n",
    "        model_filename = get_weights_file_path(config,config['preload'])\n",
    "        print(f'Preloading model {model_filename}')\n",
    "        state = torch.load(model_filename)\n",
    "        initial_epoch = state['epoch'] + 1\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        global_step = state['global_step']\n",
    "\n",
    "    # loss function\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index = tokenizer_src.token_to_id('[PAD]'),label_smoothing=0.1).to(device)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "        torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
    "        for batch in batch_iterator:\n",
    "\n",
    "            encoder_input = batch['encoder_input'].to(device) # (b, seq_len)\n",
    "            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n",
    "            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n",
    "            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n",
    "\n",
    "            # Run the tensors through the encoder, decoder and the projection layer\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n",
    "            proj_output = model.project(decoder_output) # (B, seq_len, vocab_size)\n",
    "\n",
    "            # Compare the output with the label\n",
    "            label = batch['label'].to(device) # (B, seq_len)\n",
    "\n",
    "            # Compute the loss using a simple cross entropy\n",
    "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "            # Log the loss\n",
    "            writer.add_scalar('train loss', loss.item(), global_step)\n",
    "            writer.flush()\n",
    "\n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        # Save the model at the end of every epoch\n",
    "        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'global_step': global_step\n",
    "        }, model_filename)\n",
    "\n",
    "# ## TRAIN THE MODEL\n",
    "config = get_config()\n",
    "# train_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584559be-8d29-4533-a74b-d68f378f7852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *\n",
    "def translate(sentence: str):\n",
    "    # Define the device, tokenizers, and model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    config = get_config()\n",
    "    tokenizer_src = Tokenizer.from_file(str(Path(config['tokenizer_file'].format(config['lang_src']))))\n",
    "    tokenizer_tgt = Tokenizer.from_file(str(Path(config['tokenizer_file'].format(config['lang_tgt']))))\n",
    "    model = build_transformer(tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size(), config[\"seq_len\"], config['seq_len'], d_model=config['d_model']).to(device)\n",
    "\n",
    "    # Load the pretrained weights\n",
    "    model_filename = latest_weights_file_path(config)\n",
    "    state = torch.load(r\"weights\\tmodel_19.pt\")\n",
    "    model.load_state_dict(state['model_state_dict'])\n",
    "\n",
    "    # if the sentence is a number use it as an index to the test set\n",
    "    label = \"\"\n",
    "    if type(sentence) == int or sentence.isdigit():\n",
    "        id = int(sentence)\n",
    "        ds = load_dataset(f\"{config['datasource']}\", f\"{config['lang_src']}-{config['lang_tgt']}\", split='all')\n",
    "        ds = BilingualDataset(ds, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "        sentence = ds[id]['src_text']\n",
    "        label = ds[id][\"tgt_text\"]\n",
    "    seq_len = config['seq_len']\n",
    "\n",
    "    # translate the sentence\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Precompute the encoder output and reuse it for every generation step\n",
    "        source = tokenizer_src.encode(sentence)\n",
    "        source = torch.cat([\n",
    "            torch.tensor([tokenizer_src.token_to_id('[SOS]')], dtype=torch.int64), \n",
    "            torch.tensor(source.ids, dtype=torch.int64),\n",
    "            torch.tensor([tokenizer_src.token_to_id('[EOS]')], dtype=torch.int64),\n",
    "            torch.tensor([tokenizer_src.token_to_id('[PAD]')] * (seq_len - len(source.ids) - 2), dtype=torch.int64)\n",
    "        ], dim=0).to(device)\n",
    "        source_mask = (source != tokenizer_src.token_to_id('[PAD]')).unsqueeze(0).unsqueeze(0).int().to(device)\n",
    "        encoder_output = model.encode(source, source_mask)\n",
    "\n",
    "        # Initialize the decoder input with the sos token\n",
    "        decoder_input = torch.empty(1, 1).fill_(tokenizer_tgt.token_to_id('[SOS]')).type_as(source).to(device)\n",
    "\n",
    "        # Print the source sentence and target start prompt\n",
    "        if label != \"\": print(f\"{f'ID: ':>12}{id}\") \n",
    "        print(f\"{f'SOURCE: ':>12}{sentence}\")\n",
    "        if label != \"\": print(f\"{f'TARGET: ':>12}{label}\") \n",
    "        print(f\"{f'PREDICTED: ':>12}\", end='')\n",
    "\n",
    "        # Generate the translation word by word\n",
    "        while decoder_input.size(1) < seq_len:\n",
    "            # build mask for target and calculate output\n",
    "            decoder_mask = torch.triu(torch.ones((1, decoder_input.size(1), decoder_input.size(1))), diagonal=1).type(torch.int).type_as(source_mask).to(device)\n",
    "            out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "            # project next token\n",
    "            prob = model.project(out[:, -1])\n",
    "            _, next_word = torch.max(prob, dim=1)\n",
    "            decoder_input = torch.cat([decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1)\n",
    "\n",
    "            # print the translated word\n",
    "            print(f\"{tokenizer_tgt.decode([next_word.item()])}\", end=' ')\n",
    "\n",
    "            # break if we predict the end of sentence token\n",
    "            if next_word == tokenizer_tgt.token_to_id('[EOS]'):\n",
    "                break\n",
    "\n",
    "    # convert ids to tokens\n",
    "    return tokenizer_tgt.decode(decoder_input[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a81e8f-546c-429e-8b48-f572225524ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "translate(\"Hello how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25b0978-4775-4a5b-a366-0e10ad96a058",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_39_env",
   "language": "python",
   "name": "pytorch_39_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
